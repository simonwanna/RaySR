# @package _global_
trainer:
  _target_: lightning.Trainer

  accelerator: gpu
  devices: 1
  precision: 32
  num_sanity_val_steps: 1

  max_epochs: 10
  check_val_every_n_epoch: 1

  # Logging
  log_every_n_steps: 1
  enable_progress_bar: true

  # WandB logger
  logger:
    _target_: lightning.pytorch.loggers.wandb.WandbLogger
    project: ${oc.env:WANDB_PROJECT, raysr}
    entity: ${oc.env:WANDB_ENTITY, null}
    save_dir: ${save_dir}
    name: ${now:%Y-%m-%d_%H-%M-%S}
    tags:
      - ${hydra:runtime.choices.model}
      - scale_${model.scale}
    log_model: true

  # Callbacks
  callbacks:
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ${hydra:run.dir}/checkpoints
      filename: ${hydra:runtime.choices.model}-{epoch:02d}-{val_loss:.3f}
      monitor: val_loss
      save_top_k: 3
      mode: min

    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val_loss
      patience: 15
      mode: min

    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: step

